<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jędrzej Orbik</title>
  
  <meta name="author" content="Jędrzej Orbik">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jędrzej Orbik</name>
              </p>
              <p>Currently a PhD student at TU Vienna under professor Dongheui Lee, focusing on reinforcement learning for robotics applications. Previously a research engineer at 
                <a href="http://rail.eecs.berkeley.edu">UC Berkeley</a> advised by professor Sergey Levine. Experienced robotics software engineer with a strong background in motion planning, robot perception, and deep reinforcement learning. Skilled in designing and implementing high-quality software for robotic automation systems. Proven ability to work closely with customers, scope projects, and lead development teams towards successful production deployments.
              </p>
              <p style="text-align:center">
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
                <a href="data/JedrzejOrbik-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://twitter.com/JedrzejOrbik">Twitter</a> &nbsp/&nbsp
                <a href="https://bsky.app/profile/jedrzejorbik.bsky.social">Bluesky</a> &nbsp/&nbsp
                <a href="https://github.com/jendker/">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jorbik.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jorbik.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in deep reinforcement learning for robotic manipulation, transfer learning in robotics, and efficient skill acquisition and generalization in autonomous systems. Much of my research revolves around inferring the physical world for robotic control.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                <img src="images/ariel.gif" width=160, height=160></img>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.04703">
                <papertitle>Don't Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning</papertitle>
              </a>
              <br>
              Homer Walke,
              Jonathan Yang,
              Albert Yu,
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a>,
              <strong>Jędrzej Orbik</strong>,
              <a href="https://www.avisingh.org/">Avi Singh</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2022
              <br>
              <em>RSS Workshop on Learning from Diverse, Offline Data</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.04703">arXiv</a> /
              <a href="https://sites.google.com/view/ariel-berkeley">website</a>
              <p></p>
              <p>We demonstrate that incorporating prior data into robotic reinforcement learning enables 
                 autonomous learning, substantially improves sample-efficiency of learning, and enables better 
                 generalization. Our method learns new robotic manipulation skills directly from image observations 
                 and with minimal human intervention to reset the environment. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <img src="images/relmm.png" width=160, height=160>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/relmm">
                <papertitle>Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation</papertitle>
              </a>
              <br>
              <a href="https://charlesjsun.github.io/">Charles Sun*</a>,
              <strong>Jędrzej Orbik*</strong>,
              <a href="https://twitter.com/ColinearDevin">Coline Devin</a>, 
              <a href="https://byang.org/">Brian Yang</a>,
              <a href="https://abhishekunique.github.io/">Ahbishek Gupta</a>, 
              <a href="https://www.fracturedplane.com/">Glen Berseth</a>, 
              and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <!-- Coline Devin, Brian Yang, Abhishek Gupta, Glen Berseth, and Sergey Levine -->
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2107.13545">arXiv</a> /
              <a href="https://bair.berkeley.edu/blog/2023/01/20/relmm/">blog post</a> /
              <a href="https://sites.google.com/view/relmm">website</a> /
              <a href="https://github.com/charlesjsun/ReLMM">code</a>
              <br>
              <p>
                We propose a reinforcement learning system that can learn mobile manipulation tasks continuously in the real world 
                without any environment instrumentation, without human intervention, and without access to privileged information, 
                such as maps, objects positions, or a global view of the environment.
              </p>
            </td>
          </tr> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                <img src="images/irl-dext-hand.png" width=160, height=160></img>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.04703">
                <papertitle>Inverse reinforcement learning for dexterous hand manipulation</papertitle>
              </a>
              <br>
              <strong>Jędrzej Orbik</strong>,
              <a href="https://asl.ict.tuwien.ac.at/team-dongheui-lee.html">Dongheui Lee</a>,
              <a href="https://iis.uibk.ac.at/people/agostini/">Alejandro Agostini</a>,
              <br>
              <em>IEEE International Conference on Development and Learning (ICDL)</em>, 2021
              <br>
              <a href="https://mediatum.ub.tum.de/doc/1632205/1632205.pdf">paper</a> /
              <a href="https://sites.google.com/view/irl-for-dexterous-hand">website</a> /
              <a href="https://github.com/Jendker/inverse_rl_dexterous_hand">source code</a>
              <p></p>
              <p>We identify that the learned rewards using existing IRL approaches are strongly biased towards 
                demonstrated actions due to the scarcity of samples in the vast state-action space of dexterous manipulation applications. 
                In this work we use statistical tools for random sample generation and reward normalization 
                to reduce this bias. We show that this approach improves learning stability and robustness of policies learned with the inferred reward.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two">
                <img src="images/dext-hand-retargeting.png" width=160, height=160></img>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.04703">
                <papertitle>Human hand motion retargeting for dexterous robotic hand</papertitle>
              </a>
              <br>
              <strong>Jędrzej Orbik</strong>,
              Shile Li,
              <a href="https://asl.ict.tuwien.ac.at/team-dongheui-lee.html">Dongheui Lee</a>,
              <br>
              <em>18th International Conference on Ubiquitous Robots (UR)</em>, 2021
              <br>
              <a href="https://mediatum.ub.tum.de/doc/1637161/1637161.pdf">paper</a> /
              <a href="https://sites.google.com/view/retargeting-for-dexterous-hand">website</a>
              <p></p>
              <p> We propose a low-cost framework to map the human hand motion from a single RGB-D camera to a dexterous robotic hand. 
                We incorporate neural network pose estimation and inverse kinematics for real-time hand motion retargeting. 
                Empirically, the proposed framework can successfully perform grasping task imitations.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info/">template</a>
                  </font>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>
